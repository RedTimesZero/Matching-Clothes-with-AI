import requests
import json
import time
import re

# ==================================================
# 1. åŸºæœ¬è¨­å®š
# ==================================================

SUBREDDITS = {
    "male": [
        "malefashionadvice",
        "streetwear"
    ],
    "female": [
        "femalefashionadvice",
        "OUTFITS"
    ]
}

POST_LIMIT = 2000         # æ¯å€‹ subreddit æœ€å¤šæŠ“å¹¾ç¯‡
SLEEP = 1.0               # Reddit è«‹æ±‚é–“éš”ï¼ˆä¸è¦å¤ªå¿«ï¼‰

HEADERS = {
    "User-Agent": "OutfitResearchBot/1.0 (academic project)"
}

OUTPUT_FILE = "reddit_outfit_pairs.json"

# ==================================================
# 2. é¡å‹èˆ‡é¡è‰²å­—å…¸ï¼ˆè‹±æ–‡ï¼‰
# ==================================================

TOP_KEYWORDS = {
    "t-shirt": ["t-shirt", "tee", "tshirt"],
    "shirt": ["shirt", "button up", "button-up"],
    "hoodie": ["hoodie", "sweatshirt"],
    "sweater": ["sweater", "knit", "pullover", "jumper"],
    "jacket": ["jacket", "blazer"],
    "coat": ["coat", "overcoat", "trench"],
    "cardigan": ["cardigan"],
    "top": ["top"]
}

BOTTOM_KEYWORDS = {
    "jeans": ["jeans", "denim"],
    "wide pants": ["wide pants", "wide trousers", "wide leg"],
    "pants": ["pants", "trousers", "slacks", "chinos"],
    "shorts": ["shorts"],
    "skirt": ["skirt"],
    "leggings": ["leggings"],
    "joggers": ["joggers", "sweatpants"]
}

COLORS = {
    "black": ["black"],
    "white": ["white", "cream"],
    "gray": ["gray", "grey", "charcoal"],
    "navy": ["navy"],
    "blue": ["blue"],
    "light blue": ["light blue", "baby blue"],
    "dark blue": ["dark blue"],
    "beige": ["beige", "tan", "khaki"],
    "brown": ["brown", "chocolate"],
    "green": ["green"],
    "olive": ["olive"],
    "red": ["red", "burgundy", "maroon"],
    "pink": ["pink"],
    "yellow": ["yellow"],
    "orange": ["orange"],
    "purple": ["purple"]
}

# ==================================================
# 3. æŠ½å–å·¥å…·
# ==================================================

def extract_category(text, mapping):
    for cat, kws in mapping.items():
        for kw in kws:
            if kw in text:
                return cat
    return None


def extract_all_colors(text):
    found = []
    for color, kws in COLORS.items():
        for kw in kws:
            if kw in text:
                found.append(color)
                break
    return list(dict.fromkeys(found))


# ==================================================
# 4. è§£æä¸€ç¯‡ Reddit è²¼æ–‡
# ==================================================

def parse_post(text, gender):
    text = text.lower()

    top_type = extract_category(text, TOP_KEYWORDS)
    bottom_type = extract_category(text, BOTTOM_KEYWORDS)
    colors = extract_all_colors(text)

    # å¿…é ˆæœ‰ä¸Šä¸‹è£ï¼Œä½†é¡è‰²å¯ä»¥ç”¨é è¨­å€¼
    if not top_type or not bottom_type:
        return None

    # é¡è‰²åˆ†é…ç­–ç•¥
    if len(colors) == 0:
        # æ²’æœ‰é¡è‰²è³‡è¨Šï¼Œç”¨é è¨­å€¼
        top_color = "black" if gender == "male" else "white"
        bottom_color = "black"
    elif len(colors) == 1:
        top_color = bottom_color = colors[0]
    else:
        top_color = colors[0]
        bottom_color = colors[1]

    return {
        "gender": gender,
        "top": {
            "type": top_type,
            "color": top_color
        },
        "bottom": {
            "type": bottom_type,
            "color": bottom_color
        }
    }

# ==================================================
# 5. ä¸»çˆ¬èŸ²ï¼ˆæ”¯æ´ Paginationï¼‰
# ==================================================

def crawl_reddit():
    outfits = []
    seen = set()

    for gender, subs in SUBREDDITS.items():
        for sub in subs:
            print(f"\nâ†’ Crawling r/{sub}", flush=True)
            
            total_fetched = 0
            after = None
            valid_outfits = 0
            
            while total_fetched < POST_LIMIT:
                # æ§‹å»º URLï¼ˆæ¯æ¬¡æœ€å¤š 100ï¼‰
                batch_size = min(100, POST_LIMIT - total_fetched)
                url = f"https://www.reddit.com/r/{sub}/top.json?t=all&limit={batch_size}"
                if after:
                    url += f"&after={after}"
                
                r = requests.get(url, headers=HEADERS)
                
                if r.status_code != 200:
                    print(f"  âœ– Failed (status {r.status_code})", flush=True)
                    break
                
                json_data = r.json()
                posts = json_data["data"]["children"]
                after = json_data["data"].get("after")
                
                if not posts:
                    print(f"  âš  No more posts available", flush=True)
                    break
                
                total_fetched += len(posts)
                
                for post in posts:
                    data = post["data"]
                    text = (data.get("title", "") + " " + data.get("selftext", "")).strip()
                    
                    outfit = parse_post(text, gender)
                    if not outfit:
                        continue
                    
                    key = (
                        outfit["gender"],
                        outfit["top"]["type"],
                        outfit["top"]["color"],
                        outfit["bottom"]["type"],
                        outfit["bottom"]["color"]
                    )
                    
                    if key in seen:
                        continue
                    
                    seen.add(key)
                    outfits.append(outfit)
                    valid_outfits += 1
                
                print(f"  Progress: {total_fetched} posts fetched, {valid_outfits} valid outfits", flush=True)
                
                # å¦‚æœæ²’æœ‰ä¸‹ä¸€é ï¼Œåœæ­¢
                if not after:
                    print(f"  âœ“ Reached end of subreddit", flush=True)
                    break
                
                time.sleep(SLEEP)
            
            print(f"  âœ“ Finished r/{sub}: {valid_outfits} outfits collected", flush=True)

    return outfits

# ==================================================
# 6. åŸ·è¡Œ
# ==================================================

def main():
    print("=" * 60, flush=True)
    print("ğŸš€ é–‹å§‹çˆ¬å– Reddit ç©¿æ­æ•¸æ“š...", flush=True)
    print(f"ç›®æ¨™: æ¯å€‹ subreddit æœ€å¤š {POST_LIMIT} ç¯‡", flush=True)
    print("=" * 60, flush=True)
    
    data = crawl_reddit()
    
    print("\n" + "=" * 60, flush=True)
    print(f"âœ… æ”¶é›†å®Œæˆï¼å…± {len(data)} çµ„ç¨ç‰¹ç©¿æ­é…å°", flush=True)
    print("=" * 60, flush=True)

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)

    print(f"\nğŸ’¾ å·²ä¿å­˜è‡³: {OUTPUT_FILE}", flush=True)


if __name__ == "__main__":
    main()
