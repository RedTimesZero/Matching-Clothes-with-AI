import io
import requests 
import torch
import torch.nn as nn
from torchvision import models, transforms
from fastapi import FastAPI, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from PIL import Image
import torch.nn.functional as F
import json
import os
import sys

app = FastAPI()

# --- è¨­å®š CORS ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==========================================
# 1. å®šç¾©æ¨¡å‹æ¶æ§‹ (é›™é ­é¾)
# ==========================================
class MultiHeadResNet(nn.Module):
    def __init__(self, num_cats, num_cols):
        super().__init__()
        self.backbone = models.resnet18(pretrained=False)
        num_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Identity()
        self.fc_cat = nn.Linear(num_features, num_cats)
        self.fc_color = nn.Linear(num_features, num_cols)

    def forward(self, x):
        features = self.backbone(x)
        return self.fc_cat(features), self.fc_color(features)

# ==========================================
# 2. è³‡æºç®¡ç† (å»¶é²è¼‰å…¥æ ¸å¿ƒ)
# ==========================================

# å…¨åŸŸè®Šæ•¸
classifier = None
cat_map = None
color_map = None
CLASS_NAMES = None
COLOR_NAMES = None
transform_classify = None

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# é è™•ç† (éœæ…‹å®šç¾©ï¼Œä¸åƒè¨˜æ†¶é«”)
transform_classify = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# Hugging Face API è¨­å®š
HF_API_URL = os.getenv(
    "HF_API_URL",
    "https://api-inference.huggingface.co/pipeline/feature-extraction/openai/clip-vit-base-patch32"
)
HF_API_TOKEN = os.getenv("HF_API_TOKEN")
HF_HEADERS = {"Authorization": f"Bearer {HF_API_TOKEN}"} if HF_API_TOKEN else {}

def pil_to_bytes(img):
    """Convert PIL image to bytes for HTTP upload."""
    buf = io.BytesIO()
    img.save(buf, format="PNG")
    return buf.getvalue()

def hf_image_embedding(image_bytes):
    """Call Hugging Face Inference API to get image embedding."""
    if not HF_API_TOKEN:
        raise RuntimeError("HF_API_TOKEN not set")
    headers = {"Content-Type": "application/octet-stream", **HF_HEADERS}
    resp = requests.post(HF_API_URL, headers=headers, data=image_bytes, timeout=30)
    if resp.status_code != 200:
        raise RuntimeError(f"HF API error {resp.status_code}: {resp.text}")
    data = resp.json()
    emb = torch.tensor(data[0], dtype=torch.float32)
    if emb.ndim > 1:
        emb = emb.view(emb.shape[0], -1)
    emb = emb.squeeze(0)
    return emb

def cosine_score(a, b):
    """Cosine similarity (%) between two 1-D tensors."""
    if a.ndim != 1:
        a = a.view(-1)
    if b.ndim != 1:
        b = b.view(-1)
    a = a / (a.norm(p=2) + 1e-8)
    b = b / (b.norm(p=2) + 1e-8)
    return float(torch.dot(a, b).item() * 100)

def load_class_mappings():
    """è¼‰å…¥ JSON è¨­å®šæª”"""
    json_path = os.path.join(os.path.dirname(__file__), "class_mapping.json")
    if not os.path.exists(json_path):
        print(f"âŒ æ‰¾ä¸åˆ° class_mapping.json")
        return {}, {}
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data.get('cat_map', {}), data.get('color_map', {})

def get_classifier_model():
    """
    å–å¾—åˆ†é¡æ¨¡å‹ (ResNet)ã€‚å¦‚æœé‚„æ²’è¼‰å…¥ï¼Œç¾åœ¨æ‰è¼‰å…¥ã€‚
    """
    global classifier, CLASS_NAMES, COLOR_NAMES
    
    if classifier is not None:
        return classifier, CLASS_NAMES, COLOR_NAMES

    print("âš¡ æ­£åœ¨åˆå§‹åŒ–åˆ†é¡æ¨¡å‹ (ResNet)...")
    
    # 1. è¼‰å…¥é¡åˆ¥
    c_map, co_map = load_class_mappings()
    # è½‰æ› key ç‚º int
    c_map = {int(k): v for k, v in c_map.items()}
    co_map = {int(k): v for k, v in co_map.items()}
    
    CLASS_NAMES = [c_map[i] for i in sorted(c_map.keys())]
    COLOR_NAMES = [co_map[i] for i in sorted(co_map.keys())]
    
    num_cats = len(CLASS_NAMES) if CLASS_NAMES else 1
    num_cols = len(COLOR_NAMES) if COLOR_NAMES else 1
    
    # 2. è¼‰å…¥æ¨¡å‹æ¶æ§‹
    model = MultiHeadResNet(num_cats, num_cols)
    
    # 3. å°‹æ‰¾æ¬Šé‡æª” (å®¹éŒ¯å¤§å°å¯«)
    pth_name = "Model_Weights.pth"
    if not os.path.exists(pth_name):
        pth_name = "model_weights.pth" # è©¦è©¦çœ‹å°å¯«
    
    if os.path.exists(pth_name):
        try:
            state_dict = torch.load(pth_name, map_location=device)
            model.load_state_dict(state_dict)
            model.to(device)
            model.eval()
            classifier = model
            print(f"âœ… åˆ†é¡æ¨¡å‹è¼‰å…¥æˆåŠŸ ({pth_name})")
        except Exception as e:
            print(f"âŒ æ¬Šé‡æª”è¼‰å…¥å¤±æ•—: {e}")
            classifier = None
    else:
        print("âŒ æ‰¾ä¸åˆ° Model_Weights.pth (è«‹ç¢ºèªæª”æ¡ˆå·²ä¸Šå‚³)")
        classifier = None

    return classifier, CLASS_NAMES, COLOR_NAMES

# ==========================================
# 3. API æ¥å£
# ==========================================

@app.get("/")
def home():
    return {"message": "AI Backend is Running!"}

@app.post("/predict_type")
async def predict_type(file: UploadFile = File(...)):
    # é€™è£¡åªå‘¼å« ResNetï¼Œä¸å‘¼å« CLIP
    model, classes, colors = get_classifier_model()
    
    if model is None:
        return {"category": "unknown", "color": "unknown", "error": "Model failed to load"}

    try:
        image_data = await file.read()
        image = Image.open(io.BytesIO(image_data)).convert("RGB")
        
        img_tensor = transform_classify(image).unsqueeze(0).to(device)
        
        with torch.no_grad():
            cat_logits, col_logits = model(img_tensor)
            _, cat_idx = torch.max(cat_logits, 1)
            _, col_idx = torch.max(col_logits, 1)
            
            c_idx = cat_idx.item()
            co_idx = col_idx.item()
            
            # å®‰å…¨å­˜å–
            pred_cat = classes[c_idx] if classes and c_idx < len(classes) else "unknown"
            pred_col = colors[co_idx] if colors and co_idx < len(colors) else "unknown"

        return {"category": pred_cat, "color": pred_col}
    except Exception as e:
        print(f"é æ¸¬éŒ¯èª¤: {e}")
        return {"category": "error", "color": "error"}

@app.post("/compare_url")
async def compare_url(file1: UploadFile = File(...), url2: str = Form(...)):
    print("\n" + "="*60)
    print("ğŸ“¸ æ”¶åˆ° /compare_url è«‹æ±‚")
    print(f"ğŸ”— url2: {url2[:80]}...")
    
    if not HF_API_TOKEN:
        print("âŒ HF_API_TOKEN æœªè¨­å®š")
        return {"similarity": 0, "message": "HF_API_TOKEN not set"}

    try:
        # è®€å–ä¸Šå‚³åœ–ç‰‡
        file1_data = await file1.read()
        print(f"âœ… file1 å¤§å°: {len(file1_data)} bytes")
        img1 = Image.open(io.BytesIO(file1_data)).convert("RGB")
        print(f"âœ… img1 å°ºå¯¸: {img1.size}")
        
        # ä¸‹è¼‰è¡£æ«ƒåœ–ç‰‡
        print(f"â¬‡ï¸  æ­£åœ¨ä¸‹è¼‰ url2...")
        r = requests.get(url2, timeout=10)
        r.raise_for_status()
        print(f"âœ… url2 ä¸‹è¼‰æˆåŠŸ: {len(r.content)} bytes, status={r.status_code}")
        img2 = Image.open(io.BytesIO(r.content)).convert("RGB")
        print(f"âœ… img2 å°ºå¯¸: {img2.size}")

        # å‘¼å« HF API å–å¾— embedding
        print("ğŸ¤– å‘¼å« HF API å–å¾— embedding...")
        emb1 = hf_image_embedding(pil_to_bytes(img1))
        print(f"âœ… emb1 shape: {emb1.shape}, norm: {emb1.norm().item():.4f}")
        
        emb2 = hf_image_embedding(pil_to_bytes(img2))
        print(f"âœ… emb2 shape: {emb2.shape}, norm: {emb2.norm().item():.4f}")

        # è¨ˆç®—ç›¸ä¼¼åº¦
        score = cosine_score(emb1, emb2)
        print(f"ğŸ¯ ç›¸ä¼¼åº¦åˆ†æ•¸: {score:.2f}%")
        print("="*60 + "\n")
        
        return {"similarity": score, "message": "success"}

    except Exception as e:
        print(f"âŒ æ¯”å°éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        print("="*60 + "\n")
        return {"similarity": 0, "message": str(e)}